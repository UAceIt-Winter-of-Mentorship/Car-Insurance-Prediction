# -*- coding: utf-8 -*-
"""Car_Insurance_Notebook

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Charanjeev-2k2/Car-Insurance-Prediction/blob/main/Car_Insurance_Notebook.ipynb

#Importing Libraries and Data
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("Train_data.csv") #df1 for train
df2 = pd.read_csv("Test_data.csv") #df2 for test

"""Exploratory Data Analysis (EDA)"""

df.shape

df2.shape

df.head()

df.info()

df.describe()

alpha = df.corr()
alpha["car_insurance"]

df.head(10)

df2.head(10)

"""#Data  Cleaning and Preprocessing"""

df["age"] = df['age'].astype(int)
df2["age"] = df2['age'].astype(int)

x = pd.to_datetime(df["call_start"].values, format = "%H:%M:%S")
y = pd.to_datetime(df["call_end"].values, format = "%H:%M:%S")

a = pd.to_datetime(df2["call_start"].values, format = "%H:%M:%S")
b = pd.to_datetime(df2["call_end"].values, format = "%H:%M:%S")

df.dtypes

df2.dtypes

df["call_time"] = y - x
df2["call_time"] = b-a

df.head()

df2.head()

df["call_time"] = df["call_time"].astype(int) // 10 ** 9
df2["call_time"] = df2["call_time"].astype(int) // 10 ** 9

df.dtypes

df2.dtypes

df.head()

df_miss_val = df.isnull().sum()
(df_miss_val[df_miss_val > 0]/df.shape[0]) * 100

#Percentage of missing values
df2_miss_val = df2.isnull().sum()
(df2_miss_val[df2_miss_val > 0]/df2.shape[0]) * 100

df.replace({"last_contact_month": {"jan" : "1", "feb": "2", "mar": "3", "apr": "4", "may": "5", "jun": "6", "jul": "7", "aug": "8", "sep": "9", "oct":"10", "nov": "11", "dec":"12"}}, inplace = True)
df2.replace({"last_contact_month": {"jan" : "1", "feb": "2", "mar": "3", "apr": "4", "may": "5", "jun": "6", "jul": "7", "aug": "8", "sep": "9", "oct":"10", "nov": "11", "dec":"12"}}, inplace = True)

df["date"] = df["last_contact_day"].astype(str) + "/" + df["last_contact_month"] + "/2019"
df2["date"] = df2["last_contact_day"].astype(str) + "/" + df2["last_contact_month"] + "/2019"

df.head()

df2.head()

df["days_passed"] = pd.to_datetime(df["date"].values, format = "%d/%M/%Y")
df2["days_passed"] = pd.to_datetime(df2["date"].values, format = "%d/%M/%Y")

df.describe()

df["days_passed"] = pd.to_datetime("01/01/2030", format= "%d/%M/%Y") - df["days_passed"]
df["days_passed"] = df["days_passed"].astype(int) // 10 ** 9

df2["days_passed"] = pd.to_datetime("01/01/2030", format= "%d/%M/%Y") - df2["days_passed"]
df2["days_passed"] = df["days_passed"].astype(int) // 10 ** 9

df

df2

alpha = df.corr()
alpha["car_insurance"]

train = df.drop(columns = ["Outcome", "communication", "call_start", "call_end", "last_contact_day", "last_contact_month"], inplace = False)
test = df2.drop(columns = ["Outcome", "communication", "call_start", "call_end", "last_contact_day", "last_contact_month"], inplace = False)

train.head()

test.head()

#Data Imputation
from sklearn.impute import SimpleImputer
# missing values - impute with mode (most frequent)
mode_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
mode_imputer = mode_imputer.fit(train[['job_type']])
train['job_type'] = mode_imputer.transform(train[['job_type']]).ravel()

mode_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
mode_imputer = mode_imputer.fit(train[['education_level']])
train['education_level'] = mode_imputer.transform(train[['education_level']]).ravel()


mode_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
mode_imputer = mode_imputer.fit(test[['job_type']])
test['job_type'] = mode_imputer.transform(test[['job_type']]).ravel()

mode_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
mode_imputer = mode_imputer.fit(test[['education_level']])
test['education_level'] = mode_imputer.transform(test[['education_level']]).ravel()

train.isnull().sum()

test.isnull().sum()

train.dtypes

job_type = pd.get_dummies(data=train['job_type'], drop_first=True)
marital_status = pd.get_dummies(data=train['marital_status'], drop_first=True)
education_level = pd.get_dummies(data=train['education_level'], drop_first=True)
train = pd.concat((job_type, marital_status, education_level,train), axis = 1 )


job_type = pd.get_dummies(data=test['job_type'], drop_first=True)
marital_status = pd.get_dummies(data=test['marital_status'], drop_first=True)
education_level = pd.get_dummies(data=test['education_level'], drop_first=True)
test = pd.concat((job_type, marital_status, education_level,test), axis = 1 )

print("There are {} features in train set".format(len(train.columns)))

print("There are {} features in test set".format(len(test.columns)))

missing_feature = list(set(train.columns) - set(test.columns))[0]

print(missing_feature)

train.drop(columns = ["job_type", "marital_status", "education_level","date"], inplace = True)

y = train["car_insurance"].values
X = train.drop(columns = "car_insurance", inplace = False)
X = X.values

test.drop(columns = ["job_type", "marital_status", "education_level","date"], inplace = True)

#Splitting Data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 0)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

train.dtypes

test.dtypes

"""#**Building and Evaluating different Machine Learning models**

#DTREE
"""

from sklearn.tree import DecisionTreeClassifier
DTmodel = DecisionTreeClassifier()
DTmodel.fit(X_train, y_train)
y_pred_DT = DTmodel.predict(X_test)
DTmodel.score(X_test, y_test)

from sklearn.tree import DecisionTreeClassifier
DTmodel = DecisionTreeClassifier()
DTmodel.fit(X_train, y_train)
y_pred_DT = DTmodel.predict(test)

prediction = pd.DataFrame(y_pred_DT, columns=["prediction"])

prediction.to_csv("Charanjeev_Submission.csv", index = False)

"""#KNN"""

from sklearn.neighbors import KNeighborsClassifier
KNNmodel = KNeighborsClassifier(n_neighbors = 5)
KNNmodel.fit(X_train, y_train)
y_pred_KNN = KNNmodel.predict(X_test)
KNNmodel.score(X_test, y_test)

from sklearn.neighbors import KNeighborsClassifier
KNNmodel = KNeighborsClassifier(n_neighbors = 5)
KNNmodel.fit(X_train, y_train)
y_pred_KNN = KNNmodel.predict(test)
prediction = pd.DataFrame(y_pred_KNN, columns=["prediction"])
prediction.to_csv("Charanjeev_Submission_KNN.csv", index = False)

"""#LR"""

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression(random_state = 0)
LRmodel.fit(X_train, y_train)
y_predict_LR = LRmodel.predict(X_test)
LRmodel.score(X_test, y_test)

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression(random_state = 0)
LRmodel.fit(X_train, y_train)
y_predict_LR = LRmodel.predict(test)
prediction = pd.DataFrame(y_predict_LR, columns=["prediction"])
prediction.to_csv("Charanjeev_Submission_LR.csv", index = False)

"""#SVM"""

from sklearn.svm import SVC
SVMmodel = SVC(random_state = 0)
SVMmodel.fit(X_train, y_train)
y_pred_SVM = SVMmodel.predict(X_test)
SVMmodel.score(X_test, y_test)

from sklearn.svm import SVC
SVMmodel = SVC(random_state = 0)
SVMmodel.fit(X_train, y_train)
y_pred_SVM = SVMmodel.predict(test)
prediction = pd.DataFrame(y_pred_SVM, columns=["prediction"])
prediction.to_csv("Charanjeev_Submission_SVM.csv", index = False)

"""#RF"""

from sklearn.ensemble import RandomForestClassifier
RFmodel = RandomForestClassifier(random_state = 0, n_estimators=210)
RFmodel.fit(X_train, y_train)
y_pred_RF = RFmodel.predict(X_test)
RFmodel.score(X_test, y_test)

"""Since the best Accuracy is for Random Forest Classifier.
Thus, its prediction file is used for submission

#Prediction for test dataset
"""

from sklearn.ensemble import RandomForestClassifier
RFmodel = RandomForestClassifier(random_state = 0, n_estimators=210)
RFmodel.fit(X_train, y_train)
y_predict_RF = RFmodel.predict(test)
prediction = pd.DataFrame(y_predict_RF, columns=["prediction"])
prediction.to_csv("Charanjeev_Submission_RF.csv", index = False)